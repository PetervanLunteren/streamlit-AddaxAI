# AddaxAI Video Classification Implementation Progress

## Overview
Successfully implemented video frame classification for AddaxAI using a scalable subprocess-based architecture. This allows the classification phase to work with videos containing `frame_number` fields from MegaDetector detection results.

## Problem Statement
The detection phase was updated to handle videos (see `utils/megadetector_utils.py:55`) but the classification phase only worked with images. The detection phase creates JSON results like:
```json
{
  "images": [
    {
      "file": "REC0002.mp4",
      "frame_rate": 24.0,
      "frames_processed": [0, 24, 48, 72, 96, 120],
      "detections": [
        {
          "category": "1",
          "conf": 0.838,
          "bbox": [0.5, 0.5179, 0.1273, 0.1921],
          "frame_number": 0
        }
      ]
    }
  ]
}
```

## Solution Implemented

### Architecture: Scalable Subprocess-Based Classification

**Goal**: Separate orchestration (base environment) from model inference (model-specific environments) to avoid dependency pollution.

#### Core Components Created:

1. **`classification/model_inference_wrapper.py`**
   - Generic wrapper that runs in model-specific environments
   - Dynamically loads existing model scripts (`get_crop`, `get_classification` functions)
   - Takes single image + bbox, returns JSON classification results
   - Protocol: `{"classifications": [["species_name", confidence], ...]}`

2. **`classification/video_cls_inference.py`** (completely rewritten)
   - Runs in MegaDetector environment (has OpenCV + MegaDetector video utils)
   - Extracts video frames using `megadetector.detection.video_utils.run_callback_on_frames`
   - For each frame with detections: saves temp image â†’ calls model subprocess â†’ processes results
   - Updates JSON with classification results

3. **`classification/cls_inference_subprocess.py`**
   - Subprocess-based version of regular image classification
   - Maintains same interface but uses subprocess calls for consistency
   - Replaces the original direct model loading approach

4. **`utils/video_classification.py`** (updated)
   - Coordinator that calls video classification subprocess
   - Runs video classification in MegaDetector environment (needs OpenCV)

#### Pipeline Integration:

**Updated `utils/analysis_utils.py`** to implement separate processing:
```
Videos: Detection â†’ Video Classification â†’ Results
Images: Detection â†’ Image Classification â†’ Results  
Then: Merge final results
```

**Progress Bar Setup** (max 4 bars):
- `"Detecting... (videos)"` â†’ `"Classification... (videos)"`
- `"Detecting... (images)"` â†’ `"Classification... (images)"`

## Key Architecture Benefits

âœ… **Scalable**: Works with any model environment  
âœ… **Clean separation**: Base environment (orchestration) vs model environments (inference)  
âœ… **No dependency pollution**: Each environment only has what it needs  
âœ… **Consistent**: Same subprocess pattern for videos and images  
âœ… **Reusable**: Existing model scripts work without modification  

## Files Modified/Created

### New Files:
- `classification/model_inference_wrapper.py` - Generic model subprocess interface
- `classification/video_cls_inference.py` - Video frame extraction and classification
- `classification/cls_inference_subprocess.py` - Subprocess-based image classification
- `utils/video_classification.py` - Video classification coordinator

### Modified Files:
- `utils/analysis_utils.py` - Updated pipeline to call video/image classification separately
- Progress bar setup to show separate video/image classification bars

### Removed Files:
- `utils/video_frame_extractor.py` - Removed complex temporary file approach

## Current Status

### âœ… Working:
- Model inference wrapper loads model functions correctly
- Individual model inference calls work (tested with IRA-ADS-v1 addax-yolov8 model)
- Subprocess architecture eliminates environment dependency issues
- Progress bar integration shows correct labels
- Syntax validation passes for all components

### ðŸ”§ Recently Fixed Issues:
1. **Environment Problems**: Video classification now runs in MegaDetector environment (has OpenCV)
2. **Import Errors**: Fixed Python path setup in model inference wrapper
3. **Model Loading**: Handle exceptions when model scripts try to run main classification
4. **Progress Bars**: Fixed duplicate streamlit imports causing UnboundLocalError

### ðŸ§ª Ready for Testing:
The complete video classification pipeline should now work:
1. Video detection creates JSON with `frame_number` fields
2. Video classification extracts specific frames and classifies animal detections  
3. Results are merged with image classification results
4. Final JSON contains classifications for both videos and images

## Next Steps for Testing

1. Run a deployment with videos to test the full pipeline
2. Verify that video detections get proper classification results
3. Check that the merged final JSON contains both video and image classifications
4. Monitor progress bars show correctly during video classification

## Technical Notes

- **Video Processing**: Uses MegaDetector's `run_callback_on_frames` for efficient frame extraction
- **Memory Management**: Processes one frame at a time, cleans up temporary frame files immediately
- **Error Handling**: Robust subprocess error handling with timeouts and JSON parsing
- **Model Compatibility**: Works with any existing classification model without modification

## Environment Requirements

- **Base Environment** (`env-addaxai-base`): Streamlit, orchestration logic
- **MegaDetector Environment** (`env-megadetector`): OpenCV, MegaDetector video utils, video classification
- **Model Environments** (`env-pytorch`, etc.): Specific model dependencies, model inference wrapper

The architecture successfully separates concerns while maintaining compatibility with all existing models and workflows.